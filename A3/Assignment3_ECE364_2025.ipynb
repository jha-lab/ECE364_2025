{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E32UBMT7VKMm"
      },
      "source": [
        "## Prepare python environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOYbgt4_qQGw"
      },
      "source": [
        "**Note:** The specified version of pomegranate is necessary, so please do not modify it. It may take more than 5-10 mins to install these packages, so please be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installs required packages\n",
        "!apt install libgraphviz-dev\n",
        "!pip install pomegranate==0.15.0\n",
        "!pip install matplotlib pygraphviz\n",
        "\n",
        "# Press \"Restart Runtime\" after running this cell, before going to the rest of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_lm7Q-9VKMn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryOZJYQa3PG0"
      },
      "outputs": [],
      "source": [
        "random_state=5 # use this to control randomness across runs e.g., dataset partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BASGXrOy4wat"
      },
      "source": [
        "## Preparing the Credit Card Fraud Detection Dataset (2 points)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "See [here](https://www.kaggle.com/mlg-ulb/creditcardfraud) for details of the dataset. We will post process the data to balance both the classes indicating whether the transaction is fraud or not.\n",
        "\n",
        "We will use a subset of the dataset for simplifying the experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URgO9HCN6RCl"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d5x_HWlafo"
      },
      "source": [
        "#### There are a total of 284807 entries in this dataset with no missing values. The last column indicates whether the transaction is fraud or not while remaining columns are features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOxH0lu4lSBm"
      },
      "outputs": [],
      "source": [
        "# Download and load dataset\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "if not os.path.exists('creditcard.csv'):\n",
        "    !wget https://github.com/jha-lab/ECE364_2025/raw/main/data/creditcard.zip\n",
        "with ZipFile(\"creditcard.zip\", 'r') as zObject:\n",
        "    # Extracting all the members of the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(path=\".\")\n",
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Select a subset of features\n",
        "ind_subset=np.array([0,2,3,4,5,-2,-1])\n",
        "subset_features=df.columns[ind_subset]\n",
        "df=df[subset_features]\n",
        "\n",
        "# Display the first five instances in the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ji4XY3ilWvQ"
      },
      "outputs": [],
      "source": [
        "# Check the datatype of each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqAbo9DP2dOO"
      },
      "source": [
        "#### Look at some statistics of the data using the `describe` function in pandas. See [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) details about this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWOGkp8P2abU"
      },
      "outputs": [],
      "source": [
        "# Display some statistics of the data\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrdPlwcolq0j"
      },
      "source": [
        "1. Count tells us the number of Non-empty rows in a feature.\n",
        "\n",
        "2. Mean tells us the mean value of that feature.\n",
        "\n",
        "3. Std tells us the Standard Deviation Value of that feature.\n",
        "\n",
        "4. Min tells us the minimum value of that feature.\n",
        "\n",
        "5. 25%, 50%, and 75% are the percentile/quartile of each feature.\n",
        "\n",
        "6. Max tells us the maximum value of that feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0nxPN1Rlu6h"
      },
      "source": [
        "#### Visualize the distribution of fraudulent vs genuine transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lzcll4Ylvli"
      },
      "outputs": [],
      "source": [
        "# Make a pie chart showing transaction type\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n",
        "plt.axis('equal')\n",
        "plt.ylabel('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98kh3PKrlyTN"
      },
      "outputs": [],
      "source": [
        "## Check fraudulent activity over time (note: total time is 48 hours)\n",
        "df[\"Time_Hr\"] = df[\"Time\"]/3600 # convert to hours\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\n",
        "ax1.hist(df.Time_Hr[df.Class==0],bins=48,color='g',alpha=0.5)\n",
        "ax1.set_title('Genuine')\n",
        "ax2.hist(df.Time_Hr[df.Class==1],bins=48,color='r',alpha=0.5)\n",
        "ax2.set_title('Fraud')\n",
        "plt.xlabel('Time (hrs)')\n",
        "plt.ylabel('# transactions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgaqsHMul2Bm"
      },
      "outputs": [],
      "source": [
        "# Remove 'Time' feature as it is already captured when converting to hours\n",
        "df = df.drop(['Time'],axis=1)\n",
        "FEATURE_NAMES=df.drop('Class',axis=1).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzfjIdHTl5x4"
      },
      "source": [
        "#### Create a balanced dataset with 50% from each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG2j3_FAl6jR"
      },
      "outputs": [],
      "source": [
        "fraud_indices = np.array(df[df.Class == 1].index) #indices corresponding to fraud transaction\n",
        "genuine_ind = df[df.Class == 0].index #indices corresponding to genuine transaction\n",
        "total_fraud_transactions = len(df[df.Class == 1]) # total transactions that were fraud\n",
        "np.random.seed(0) # fix the random seed generator for consistent results\n",
        "indices_genuine_transaction = np.random.choice(genuine_ind, total_fraud_transactions, replace = False)\n",
        "indices_genuine_transaction = np.array(indices_genuine_transaction)\n",
        "selected_balanced_indices = np.concatenate([fraud_indices,indices_genuine_transaction]) # indices for balanced data\n",
        "balanced_data = df.iloc[selected_balanced_indices,:]\n",
        "\n",
        "print(\"% genuine transactions: \",len(balanced_data[balanced_data.Class == 0])/len(balanced_data))\n",
        "print(\"% fraud transactions: \",len(balanced_data[balanced_data.Class == 1])/len(balanced_data))\n",
        "\n",
        "# Make a pie chart showing transaction type\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.pie(balanced_data.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['green','red'])\n",
        "plt.axis('equal')\n",
        "plt.ylabel('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRhEcln77rIK"
      },
      "source": [
        "### Extract target and descriptive features (1 point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blhp_Upk8E-Y"
      },
      "outputs": [],
      "source": [
        "# Store all the features from the balanced data in X (use balanced_data, not df)\n",
        "X= # insert your code here\n",
        "# Store all the labels in y (use balanced_data, not df)\n",
        "y= # insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **NOTE**: After dropping the 'Time' and the 'Class' feature, the dataset has changed and 'Time_Hr' was added towards the end. Let's print the `X.info` again to see the current indices and info of the columns (before converting it to a numpy array):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5XDDMPY6t9U"
      },
      "outputs": [],
      "source": [
        "# Convert data to numpy array\n",
        "X = # insert your code here\n",
        "y = # insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-JPYSnc8JQi"
      },
      "source": [
        "### Create training and test datasets (1 point)\n",
        "\n",
        "\n",
        "We will split the dataset into training and test set. Generally in machine learning, we split the data into training,\n",
        "validation and test set (this will be covered in later chapters). The model with best performance on the validation set is used to evaluate perfromance on \n",
        "the test set which is the unseen data. In this assignment, we will using `train set` for training and evaluate the performance on the `test set` for various \n",
        "model configurations to determine the best hyperparameters (parameter setting yielding the best performance).\n",
        "\n",
        "Split the data into training and test set using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzTzI3iT8R5x"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = # insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Mh__LfGITs"
      },
      "source": [
        "## Checking the dependency between features (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJvUqGqkGMEM"
      },
      "source": [
        "We want to check whether or not the two features, $V3$ and $V4$, are independent from each other. For simplicity, we probe $P(-0.5 \\leq V3 \\leq 0.5)$, $P(-0.5 \\leq V4 \\leq 0.5)$, and $P(-0.5 \\leq V3 \\leq 0.5, -0.5 \\leq V4 \\leq 0.5)$ to represent $P(V3)$, $P(V4)$, and $P(V3, V4)$, respectively. Calculate them and comment on the dependency between $V3$ and $V4$. \n",
        "\n",
        "Remember that for independent features $A$ and $B$:\n",
        "\n",
        "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
        "\n",
        "If $P(A \\cap B) \\approx P(A) \\cdot P(B)$, then $A$ and $B$ can be considered independent. \n",
        "\n",
        "**Note:** Since $V3$ and $V4$ are continuous (float) features, we cannot directly calculate probabilities of exact values. Instead, we estimate probabilities by calculating the proportion of data points falling within specified ranges. The intersection $P(V3 \\cap V4)$ represents the probability that both conditions are satisfied simultaneously.\n",
        "\n",
        "**Hint:** Since `X` is a numpy array, you can use the bitwise AND (`&`) operator to combine multiple conditions. For example, `(X[:, 1] >= -0.5) & (X[:, 1] <= 0.5)` creates a boolean array indicating which rows have column 1 (V3) values in the range $[-0.5, 0.5]$. You can count True values using `.sum()`, then divide by the total number of rows `len(X)` to get the probability. \n",
        "\n",
        "**Hint:**  V3 is at index 1 and V4 is at index 2 in the numpy array `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-8enYxFGMu-"
      },
      "outputs": [],
      "source": [
        "# insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bud8WPbGUCu"
      },
      "source": [
        "**ANS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQTCntOk6t9W"
      },
      "source": [
        "## Training probability-based classifiers (16 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDcVqRS56t9X"
      },
      "source": [
        "### Exercise 1: Learning a Naive Bayes Model (7 points)\n",
        "\n",
        "#### We will use the `pomegranate` library to train a Naive Bayes Model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/v0.8.1/NaiveBayes.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjrqGWte6t9X"
      },
      "outputs": [],
      "source": [
        "from pomegranate.distributions import NormalDistribution, ExponentialDistribution, DiscreteDistribution\n",
        "from pomegranate.NaiveBayes import NaiveBayes\n",
        "from pomegranate.BayesClassifier import BayesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import math\n",
        "\n",
        "np.random.seed(random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkS5cL56t9Y"
      },
      "source": [
        "#### Exercise 1a: Fit naive bayes model using a single distribution type (1 point)\n",
        "\n",
        "#### Train one naive bayes model using a normal distribution for each feature. Train another naive bayes model using an exponential distribution for each feature. Hint: use NormalDistribution or ExponentialDistribution (continuous features) and NaiveBayes.from_samples() to fit the model to the data.\n",
        "\n",
        "#### Report the training and test set accuracies for each model. Hint: use accuracy_score()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeAiEwIs6t9Y"
      },
      "outputs": [],
      "source": [
        "# insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnm7xIL86t9Z"
      },
      "source": [
        "#### Exercise 1b: Fit a naive bayes model using different feature distributions (2 points)\n",
        "\n",
        "#### Visualize the feature distributions (done for you below) to determine which distribution (normal or exponential) better models a specific feature. \n",
        "\n",
        "#### Train a Naive Bayes classifier using this set of feature-specific distributions. Hint: use NormalDistribution or ExponentialDistribution and NaiveBayes.from_samples() to fit the model to the data.\n",
        "\n",
        "#### Report the training and test set accuracies for the model. Hint: use accuracy_score()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mREYMrmg6t9Z"
      },
      "outputs": [],
      "source": [
        "# visualization code\n",
        "\n",
        "num_cols=3\n",
        "num_rows=int(len(FEATURE_NAMES)/num_cols) if len(FEATURE_NAMES)%num_cols == 0 else int(math.ceil(len(FEATURE_NAMES)/num_cols))\n",
        "fig,ax=plt.subplots(num_rows,num_cols,figsize=(10,6))\n",
        "\n",
        "for ft_index in np.arange(X_train.shape[1]):\n",
        "    ax[ft_index//num_cols,ft_index%num_cols].hist(X_train[:,ft_index], color='blue')\n",
        "    ax[ft_index//num_cols,ft_index%num_cols].hist(X_test[:,ft_index], color='red')\n",
        "    ax[ft_index//num_cols,ft_index%num_cols].set_title(FEATURE_NAMES[ft_index])\n",
        "    \n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lZTGhq16t9Z"
      },
      "outputs": [],
      "source": [
        "# insert your code here: train a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJqOu9je6t9a"
      },
      "source": [
        "#### Comment on any performance difference between this model and the models trained in Ex. 1a. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsYqGS4k6t9a"
      },
      "source": [
        "**ANS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVA5ZzhI6t9b"
      },
      "source": [
        "#### Exercise 1c: Fit a naive bayes model on categorical features (2 points)\n",
        "\n",
        "#### Besides fitting a naive bayes model on the continuous features, one can fit a naive bayes model on categorical features derived from binning the continuous features, and then compute a probability mass function for each categorical feature.\n",
        "\n",
        "#### Bin the features by varying the strategy among {equal-width binning, equal-frequency binning}. For each binning strategy, vary the number of bins among {3,10,50}. Hint: use [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer.get_params) by modifying n_bins and strategy and setting encode=\"ordinal\" to map the labels to numerical categories.\n",
        "\n",
        "#### For each binning setting tried above, fit a naive bayes model on the binned version of the training set. Hint: use DiscreteDistribution to model the categorical features and NaiveBayes.from_samples() to fit the model to the data.\n",
        "\n",
        "#### Report the training and test set accuracy for each model trained and evaluated on binned versions of the training and test sets respectively. \n",
        "\n",
        "**Note** There may be some variability in the actual performance scores, but the overall trends should remain the consistent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oEKSVJ86t9b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu_rVaWw6t9b"
      },
      "source": [
        "#### Briefly explain any performance difference between equal-width and equal-frequency binning. Also comment on the effect of increasing the number of bins (see ch.3). (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eP9xnHa6t9c"
      },
      "source": [
        "**ANS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP6z3W4K6t9c"
      },
      "source": [
        "### Exercise 2: Learning a Bayes Net (9 points)\n",
        "\n",
        "#### We will use the `pomegranate` library to train a Bayes Net to assess whether relaxing the assumption in Naive bayes (i.e., all features are independent given the target feature) could improve the classification model. Review ch.6 and see [here](https://pomegranate.readthedocs.io/en/v0.8.1/BayesianNetwork.html) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6LhAf7J6t9c"
      },
      "source": [
        "#### Exercise 2a: Create a categorical version of the dataset (1 point)\n",
        "\n",
        "#### Create categorical versions of the training and test sets by using equal-frequency binning with the number of bins set to 10 (as in Ex. 1c). \n",
        "\n",
        "#### *<u>Use these datasets for training and evaluating the bayes net models in the following exercises.<u>*\n",
        "\n",
        "**Note** This is done because pomegranate currently only supports bayes net over categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwTP_x2I6t9c"
      },
      "outputs": [],
      "source": [
        "# insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqPAKILl6t9c"
      },
      "source": [
        "#### Exercise 2b: Construct a Bayes net (3 points)\n",
        "\n",
        "#### Construct and train a Bayes net in which the V2 node is a parent of the target feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
        "\n",
        "#### Construct and train another Bayes net in which the Time_hr node is a parent of the target feature node (only these 2 nodes should be in the net). Use construct_and_train_bayes_net (defined below) by passing in the binned training dataset and specifying the index of the parent feature node.\n",
        "\n",
        "#### Report the training and test accuracies of each Bayes Net. Use get_performance (defined below) by passing in the trained bayes net, binned datasets, and specifying the index of the parent feature node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssRrH2un6t9d"
      },
      "outputs": [],
      "source": [
        "from pomegranate import *\n",
        "\n",
        "\"\"\"\n",
        "X_train_binned: ndarray (# instances, # features) This is the binned version of the training set\n",
        "y_train: 1darray (# instances,)\n",
        "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
        "                            These indices correspond to features that are parent nodes of the target node. \n",
        "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
        "                            These indices correspond to features that are children nodes of the target node.\n",
        "                            \n",
        "Returns a BayesianNetwork representing the trained bayes net\n",
        "\"\"\"\n",
        "def construct_and_train_bayes_net(X_train_binned,\n",
        "                                  y_train,\n",
        "                                  ind_chosen_parent_features=np.array([]), \n",
        "                                  ind_chosen_child_features=np.array([]),\n",
        "                                ):\n",
        "    # parent nodes of target\n",
        "\n",
        "    dist_by_parent_feature=[]\n",
        "    state_by_parent_feature=[]\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        parent_feature_names_chosen=FEATURE_NAMES[ind_chosen_parent_features]\n",
        "\n",
        "        for ft_index in ind_chosen_parent_features:\n",
        "            ft_dist=DiscreteDistribution.from_samples(X_train_binned[:,ft_index])\n",
        "            dist_by_parent_feature.append(ft_dist)\n",
        "            state_by_parent_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
        "        dist_by_parent_feature=np.array(dist_by_parent_feature)\n",
        "        state_by_parent_feature=np.array(state_by_parent_feature)\n",
        "\n",
        "\n",
        "    # target node\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_train_parent_features_binned_with_labels=np.concatenate((X_train_binned[:,ind_chosen_parent_features],\n",
        "                                                                   np.expand_dims(y_train,axis=1)),axis=1)\n",
        "        target_dist=ConditionalProbabilityTable.from_samples(X_train_parent_features_binned_with_labels)\n",
        "        # temporary workaround to properly initialize the distribution\n",
        "        target_dist=ConditionalProbabilityTable(target_dist.parameters[0],dist_by_parent_feature.tolist())\n",
        "    else:\n",
        "        target_dist=DiscreteDistribution.from_samples(y_train)\n",
        "    target_state=State(target_dist, \"target\")\n",
        "\n",
        "    # children node of target\n",
        "\n",
        "    dist_by_child_feature=[]\n",
        "    state_by_child_feature=[]    \n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        child_feature_names_chosen=FEATURE_NAMES[ind_chosen_child_features]\n",
        "\n",
        "        for ft_index in ind_chosen_child_features:\n",
        "            X_train_child_features_binned_with_labels=np.concatenate((np.expand_dims(y_train,axis=1),\n",
        "                                                                        np.expand_dims(X_train_binned[:,ft_index],axis=1)),\n",
        "                                                                     axis=1)\n",
        "            ft_dist=ConditionalProbabilityTable.from_samples(X_train_child_features_binned_with_labels)\n",
        "            ft_dist=ConditionalProbabilityTable(ft_dist.parameters[0],[target_dist])\n",
        "            dist_by_child_feature.append(ft_dist)\n",
        "            state_by_child_feature.append(State(ft_dist, str(FEATURE_NAMES[ft_index])))\n",
        "        dist_by_child_feature=np.array(dist_by_child_feature)\n",
        "        state_by_child_feature=np.array(state_by_child_feature)\n",
        "\n",
        "\n",
        "    pom_model = BayesianNetwork()\n",
        "    pom_model.add_states(*list(state_by_parent_feature))\n",
        "    pom_model.add_states(target_state)\n",
        "    pom_model.add_states(*list(state_by_child_feature))\n",
        "\n",
        "    for parent_index in np.arange(len(ind_chosen_parent_features)):\n",
        "        pom_model.add_edge(state_by_parent_feature[parent_index],target_state)\n",
        "\n",
        "    for child_index in np.arange(len(ind_chosen_child_features)):\n",
        "        pom_model.add_edge(target_state, state_by_child_feature[child_index])\n",
        "\n",
        "    pom_model.bake()\n",
        "\n",
        "    return pom_model\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "pom_model: BayesianNetwork represents the trained bayes net model\n",
        "X_train_binned: ndarray (# instances, # features) This is the binned training set\n",
        "y_train: 1darray (# instances,)\n",
        "X_test_binned: ndarray (# instances, # features) This is the binned test set\n",
        "y_test: 1darray (# instances,)\n",
        "ind_chosen_parent_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
        "                            These indices correspond to features that are parent nodes of the target node. \n",
        "ind_chosen_child_features: 1d numpy array encodes the indices of the features relative to FEATURE_NAMES. \n",
        "                            These indices correspond to features that are children nodes of the target node.\n",
        "                            \n",
        "Returns the training and test set accuracies attained by the bayes net model (pom_model)\n",
        "\"\"\"\n",
        "def get_performance(pom_model, X_train_binned, y_train, X_test_binned, y_test, \n",
        "                    ind_chosen_parent_features=np.array([]), ind_chosen_child_features=np.array([])):\n",
        "    nones_array=np.expand_dims(np.array([None]*len(X_train_binned)),axis=1)\n",
        "    ind_target_node=len(ind_chosen_parent_features)\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_train_binned_with_none=X_train_binned[:,ind_chosen_parent_features]\n",
        "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,nones_array),axis=1)\n",
        "    else:\n",
        "        X_train_binned_with_none=nones_array\n",
        "\n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        X_train_binned_with_none=np.concatenate((X_train_binned_with_none,\n",
        "                                                X_train_binned[:,ind_chosen_child_features]),\n",
        "                                               axis=1)\n",
        "    pred_labels=np.array(pom_model.predict(X_train_binned_with_none),dtype='int64')[:,ind_target_node]\n",
        "    train_acc=accuracy_score(y_train, pred_labels)\n",
        "\n",
        "    nones_array=np.expand_dims(np.array([None]*len(X_test_binned)),axis=1)\n",
        "    if len(ind_chosen_parent_features)>0:\n",
        "        X_test_binned_with_none=X_test_binned[:,ind_chosen_parent_features]\n",
        "        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,nones_array),axis=1)\n",
        "    else:\n",
        "        X_test_binned_with_none=nones_array\n",
        "\n",
        "    if len(ind_chosen_child_features)>0:\n",
        "        X_test_binned_with_none=np.concatenate((X_test_binned_with_none,\n",
        "                                               X_test_binned[:,ind_chosen_child_features]),\n",
        "                                               axis=1)\n",
        "    pred_labels=np.array(pom_model.predict(X_test_binned_with_none),dtype='int64')[:,ind_target_node]\n",
        "    test_acc=accuracy_score(y_test, pred_labels)\n",
        "    \n",
        "    return train_acc, test_acc\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCtb-dPC6t9d"
      },
      "outputs": [],
      "source": [
        "# insert your code here (for the case where V2 is the parent node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# insert your code here (for the case where Time_hr is the parent node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wa1O-r86t9e"
      },
      "source": [
        "#### Comment on which feature seems more informative for the prediction task. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q1tGjCf6t9e"
      },
      "source": [
        "**ANS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AVRTJ8T6t9e"
      },
      "source": [
        "#### Exercise 2c: Construct a Bayes net with multiple parent nodes (3 points)\n",
        "\n",
        "Here, we'll implement a Bayes net with both parent nodes and children nodes.\n",
        "\n",
        "\n",
        "\n",
        "#### Construct and train a Bayes net in which:\n",
        "#### -the following features are all parents of the target feature node (V2, V3, Time_Hr).  \n",
        "#### -the following features are all children of the target feature node (V4, V5, Amount).  \n",
        "\n",
        "#### Use construct_and_train_bayes_net by passing in the binned training dataset and specifying the indices of the parent feature nodes AND the children feature nodes. (this could take several minutes)\n",
        "\n",
        "#### Report the training and test accuracy of the Bayes Net using get_performance by passing in the trained bayes net, binned datasets, and indices of the paren, children feature nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc3eM-mI6t9e"
      },
      "outputs": [],
      "source": [
        "# insert your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2_t0DHR6t9e"
      },
      "source": [
        "#### Compare the performance of this Bayes net against the Bayes nets from Ex. 2b. (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNdYYylX6t9e"
      },
      "source": [
        "**ANS:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
